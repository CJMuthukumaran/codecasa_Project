import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
df = pd.read_csv('EURUSD_csv.txt',index_col = 'Date')#data access
print(df)# to print data
data = df.filter(['Close'])#taking only close value
print(data)#printing close value as updated data
plt.figure(figsize = (20,8))#size of graph
plt.plot(data,'b',label = 'Original')#to plot data
plt.xlabel("Days")#to name the x axis
plt.ylabel('Price')#to name the y axis
plt.title("EURUSD_1d")# title of the graph
plt.legend()#legend
plt.show()# print graph

from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler(feature_range=(0,1))#data value is scaled between 0-1 
scaled_data = scaler.fit_transform(data)#in this the process is done 

print(scaled_data)#print data array

training_size = int(len(scaled_data)*0.80) #Training size is 80% of the given data
print("Training_size:",training_size)#printing size
x_train_1 = scaled_data[0:training_size,:]#o to training_size-1 for train data
print(len(x_train_1))
test_data_1= scaled_data[training_size:,:1]#training_size to last data for test data
print(len(test_data_1))

print(x_train_1), print(test_data_1)

def split_sequence(sequence, n_steps):
    X, y = list(), list()    
    for i in range(len(sequence)):
       # find the end of this pattern
       end_ix = i + n_steps
       # check if we are beyond the sequence
       if end_ix > len(sequence)-1:
          break
       # gather input and output parts of the pattern
       seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]
       X.append(seq_x)#updating list x
       y.append(seq_y)#updating list y
    return np.array(X), np.array(y)

time_step = 120 #assigning time step
x_train, y_train = split_sequence(x_train_1, time_step)#calling the function for training set 
x_test, y_test = split_sequence(test_data_1, time_step)#calling the function for testing set 

print(x_train.shape),print(y_train.shape)

print(x_test[-1]), print(y_test[-2:])

x_train =x_train.reshape(x_train.shape[0],x_train.shape[1] , 1)
x_test = x_test.reshape(x_test.shape[0],x_test.shape[1] , 1)
print(x_train.shape, x_test.shape)

import math
from keras.models import Sequential
from keras.layers import Dense, Activation, Conv1D ,MaxPooling1D, Dropout
from tensorflow.keras.layers import LSTM
from tensorflow.keras.utils import plot_model
from keras.metrics import RootMeanSquaredError as rmse
from keras import optimizers

model = Sequential()
model.add(Conv1D(filters=256, kernel_size=2, activation='relu',padding = 'same',input_shape=(120,1)))
model.add(MaxPooling1D(pool_size=2))
model.add(LSTM(100, return_sequences = True))
model.add(Dropout(0.3))
model.add(LSTM(100, return_sequences = False))
model.add(Dropout(0.3))
model.add(Dense(units=1, activation = 'sigmoid'))
model.compile(optimizer='adam', loss= 'mse' , metrics = [rmse()])

model.summary()
#plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

history = model.fit(x_train, y_train, epochs = 1 , validation_data = (x_test,y_test), batch_size=32, verbose=1)

history.history.keys()

plt.figure(figsize = (10,6))
plt.plot(history.history['loss'],label='Training Loss',color='b')
plt.plot(history.history['val_loss'],label='Validation-loss',color='g')
plt.xlabel("Iteration")
plt.ylabel("Loss")
plt.title('LOSS')
plt.legend()
history.history.keys()
plt.figure(figsize = (10,6))
plt.plot(history.history['loss'],label='Training Loss',color='b')
plt.plot(history.history['val_loss'],label='Validation-loss',color='g')
plt.xlabel("Iteration")
plt.ylabel("Loss")
plt.title('LOSS')
plt.legend()
plt.figure(figsize = (10,6))
plt.plot(history.history['root_mean_squared_error'],label='Training RMSE',color='b')
plt.plot(history.history['val_root_mean_squared_error'],label='Validation-RMSE',color='g')
plt.xlabel("Iteration")
plt.ylabel("RMSE")
plt.title('RMSE')
plt.legend()
model.evaluate(x_train,y_train, batch_size = 32)

model.evaluate(x_test,y_test, batch_size = 32)

train_predict = model.predict(x_train)
plot_y_train = y_train.reshape(-1,1)

# Actual vs predicted training data graph
plt.figure(figsize=(22,7))
plt.plot(scaler.inverse_transform(plot_y_train),color = 'b', label = 'Original')
plt.plot(scaler.inverse_transform(train_predict),color='red', label = 'Predicted')
plt.title("Prediction Graph Using Training Data")
plt.xlabel("Days")
plt.ylabel('Prices')
plt.legend()
plt.show()

test_predict = model.predict(x_test)
plot_y_test = y_test.reshape(-1,1)

# Actual vs predicted testing data graph
plt.figure(figsize=(22,7))
plt.plot(scaler.inverse_transform(plot_y_test),color = 'b',  label = 'Original')
plt.plot(scaler.inverse_transform(test_predict),color='g', label = 'Predicted')
plt.title("Prediction Graph Using Testing Data")
plt.xlabel("Days")
plt.ylabel('Prices')
plt.legend()
plt.show()

last_actual_five = scaler.inverse_transform(y_test[-5:])
last_predicted_five = scaler.inverse_transform(test_predict[-5:])

compare = pd.DataFrame(last_actual_five, columns = ['Actual_Prices'])
compare['Predicted_Prices'] = last_predicted_five

print(compare)

look_back = time_step
trainPredictPlot = np.empty_like(data)
trainPredictPlot[:,:] = np.nan
trainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict
# shift test predictions for plotting
testPredictPlot = np.empty_like(data)
testPredictPlot[:, :] = np.nan
testPredictPlot[len(train_predict)+(look_back*2):len(data), :] = test_predict
# plot baseline and predictions
plt.figure(figsize=(22,8))
plt.plot(data,color = 'b', label = 'Original')
plt.plot(scaler.inverse_transform(trainPredictPlot),color='red',label = 'Train Predictions')
plt.plot(scaler.inverse_transform(testPredictPlot),color='green', label = 'Testing Predictions')
plt.title("Original Price Vs Predictions")
plt.xlabel('Period')
plt.ylabel('Prices')
plt.legend()
plt.show()

model.save('my_model.keras')


